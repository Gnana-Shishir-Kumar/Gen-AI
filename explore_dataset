{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d2cfdb3",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-31T20:21:56.515374Z",
     "iopub.status.busy": "2025-10-31T20:21:56.515040Z",
     "iopub.status.idle": "2025-10-31T20:21:58.467141Z",
     "shell.execute_reply": "2025-10-31T20:21:58.465989Z"
    },
    "papermill": {
     "duration": 1.958083,
     "end_time": "2025-10-31T20:21:58.469272",
     "exception": false,
     "start_time": "2025-10-31T20:21:56.511189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "/kaggle/input/human-v-ai/5000human_5000machine.csv\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22371b8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T20:21:58.475453Z",
     "iopub.status.busy": "2025-10-31T20:21:58.474981Z",
     "iopub.status.idle": "2025-10-31T20:21:59.375914Z",
     "shell.execute_reply": "2025-10-31T20:21:59.374705Z"
    },
    "papermill": {
     "duration": 0.90596,
     "end_time": "2025-10-31T20:21:59.377860",
     "exception": false,
     "start_time": "2025-10-31T20:21:58.471900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: /kaggle/input/human-v-ai/5000human_5000machine.csv\n"
     ]
    }
   ],
   "source": [
    "csv_files = []\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.csv'):\n",
    "            csv_files.append(os.path.join(dirname, filename))\n",
    "\n",
    "if csv_files:\n",
    "    csv_path = csv_files[0]  # Use the first CSV found\n",
    "    print(f\"Loading: {csv_path}\")\n",
    "else:\n",
    "    print(\"No CSV file found! Please add the dataset to your notebook.\")\n",
    "    print(\"Click 'Add Data' button and search for 'human-v-ai' or upload the dataset\")\n",
    "    csv_path = None\n",
    "\n",
    "# Load the data\n",
    "if csv_path:\n",
    "    df = pd.read_csv(csv_path)\n",
    "else:\n",
    "    print(\"\\nPlease add the dataset first!\")\n",
    "    df = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dff6ea3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T20:21:59.385513Z",
     "iopub.status.busy": "2025-10-31T20:21:59.385164Z",
     "iopub.status.idle": "2025-10-31T20:21:59.393091Z",
     "shell.execute_reply": "2025-10-31T20:21:59.391832Z"
    },
    "papermill": {
     "duration": 0.012972,
     "end_time": "2025-10-31T20:21:59.394829",
     "exception": false,
     "start_time": "2025-10-31T20:21:59.381857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DATASET OVERVIEW\n",
      "==================================================\n",
      "Total samples: 10000\n",
      "Columns: ['index', 'text', 'label', 'source', 'text_word_count', 'label_cat', 'source_int', 'processed_text']\n",
      "\n",
      "Data types:\n",
      "index               int64\n",
      "text               object\n",
      "label               int64\n",
      "source             object\n",
      "text_word_count     int64\n",
      "label_cat          object\n",
      "source_int          int64\n",
      "processed_text     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Basic Information\n",
    "print(\"=\" * 50)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8731bc5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T20:21:59.400955Z",
     "iopub.status.busy": "2025-10-31T20:21:59.400594Z",
     "iopub.status.idle": "2025-10-31T20:21:59.635351Z",
     "shell.execute_reply": "2025-10-31T20:21:59.634094Z"
    },
    "papermill": {
     "duration": 0.239715,
     "end_time": "2025-10-31T20:21:59.636938",
     "exception": false,
     "start_time": "2025-10-31T20:21:59.397223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "MISSING VALUES\n",
      "==================================================\n",
      "index              0\n",
      "text               0\n",
      "label              0\n",
      "source             0\n",
      "text_word_count    0\n",
      "label_cat          0\n",
      "source_int         0\n",
      "processed_text     0\n",
      "dtype: int64\n",
      "\n",
      "==================================================\n",
      "FIRST 5 ROWS\n",
      "==================================================\n",
      "   index                                               text  label  source  \\\n",
      "0     47  All the Love you do not see\\r\\n\\r\\nOn writing ...      0  medium   \n",
      "1     85  Reading, writing and displaying images\\r\\n\\r\\n...      0  medium   \n",
      "2    255  Why I Think “Target Audience” Doesn’t Always M...      0  medium   \n",
      "3    268  All sorts of books are published every year, a...      0  medium   \n",
      "4    315  Finding The Right Question Not Answer\\r\\n\\r\\nW...      0  medium   \n",
      "\n",
      "   text_word_count label_cat  source_int  \\\n",
      "0              236     human           0   \n",
      "1              386     human           0   \n",
      "2              323     human           0   \n",
      "3              176     human           0   \n",
      "4              264     human           0   \n",
      "\n",
      "                                      processed_text  \n",
      "0  love see write heart hope photo joshua coleman...  \n",
      "1  read write display image anything computer vis...  \n",
      "2  think target audience always matter much first...  \n",
      "3  sort book publish every year sort bookbuyers l...  \n",
      "4  find right question answer face problem one fi...  \n",
      "\n",
      "==================================================\n",
      "LABEL DISTRIBUTION\n",
      "==================================================\n",
      "\n",
      "label distribution:\n",
      "label\n",
      "0    5000\n",
      "1    5000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source distribution:\n",
      "source\n",
      "medium           3025\n",
      "Rotten_Tomato    2635\n",
      "WMT-16           2365\n",
      "wikipedia        1975\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==================================================\n",
      "TEXT STATISTICS\n",
      "==================================================\n",
      "Text length statistics:\n",
      "count    10000.000000\n",
      "mean      1622.892600\n",
      "std        773.029222\n",
      "min        238.000000\n",
      "25%        966.000000\n",
      "50%       1716.500000\n",
      "75%       2223.000000\n",
      "max      15171.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "Word count statistics:\n",
      "count    10000.000000\n",
      "mean       274.620300\n",
      "std        126.197044\n",
      "min         51.000000\n",
      "25%        162.000000\n",
      "50%        286.000000\n",
      "75%        395.000000\n",
      "max        639.000000\n",
      "Name: word_count, dtype: float64\n",
      "\n",
      "Sample text (first 200 characters):\n",
      "All the Love you do not see\r\n",
      "\r\n",
      "On writing with heart and hope\r\n",
      "\r\n",
      "Photo by JOSHUA COLEMAN on Unsplash\r\n",
      "\r\n",
      "The words I do not recognize are the ones my heart wrote, so feverish with hope every line feels...\n",
      "\n",
      "==================================================\n",
      "DATASET READY FOR TESTING\n",
      "==================================================\n",
      "✓ Dataset loaded successfully\n",
      "✓ 10000 samples available for Winston AI testing\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Check for missing values\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\" * 50)\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Step 4: View first few rows\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FIRST 5 ROWS\")\n",
    "print(\"=\" * 50)\n",
    "print(df.head())\n",
    "\n",
    "# Step 5: Check the distribution of labels (Human vs AI)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"LABEL DISTRIBUTION\")\n",
    "print(\"=\" * 50)\n",
    "# This will depend on how the labels are stored in the CSV\n",
    "# Common column names: 'label', 'class', 'type', 'generated', 'source'\n",
    "for col in df.columns:\n",
    "    if col.lower() in ['label', 'class', 'type', 'generated', 'source', 'is_ai', 'human']:\n",
    "        print(f\"\\n{col} distribution:\")\n",
    "        print(df[col].value_counts())\n",
    "\n",
    "# Step 6: Text statistics\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"TEXT STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "# Find the text column\n",
    "text_col = None\n",
    "for col in df.columns:\n",
    "    if col.lower() in ['text', 'content', 'passage', 'essay', 'article']:\n",
    "        text_col = col\n",
    "        break\n",
    "\n",
    "if text_col:\n",
    "    df['text_length'] = df[text_col].astype(str).apply(len)\n",
    "    df['word_count'] = df[text_col].astype(str).apply(lambda x: len(x.split()))\n",
    "    \n",
    "    print(f\"Text length statistics:\")\n",
    "    print(df['text_length'].describe())\n",
    "    print(f\"\\nWord count statistics:\")\n",
    "    print(df['word_count'].describe())\n",
    "    \n",
    "    print(f\"\\nSample text (first 200 characters):\")\n",
    "    print(df[text_col].iloc[0][:200] + \"...\")\n",
    "else:\n",
    "    print(\"Could not automatically identify text column\")\n",
    "    print(\"Available columns:\", list(df.columns))\n",
    "\n",
    "# Step 7: Export summary\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"DATASET READY FOR TESTING\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"✓ Dataset loaded successfully\")\n",
    "print(f\"✓ {len(df)} samples available for Winston AI testing\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8621598,
     "sourceId": 13571611,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.641844,
   "end_time": "2025-10-31T20:22:00.159518",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-31T20:21:51.517674",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
