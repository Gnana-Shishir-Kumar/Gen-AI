{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13571611,"sourceType":"datasetVersion","datasetId":8621598}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n    print(\"\\n\" + \"=\" * 50)\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-31T20:19:42.638543Z","iopub.execute_input":"2025-10-31T20:19:42.638899Z","iopub.status.idle":"2025-10-31T20:19:42.651426Z","shell.execute_reply.started":"2025-10-31T20:19:42.638872Z","shell.execute_reply":"2025-10-31T20:19:42.650311Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\n/kaggle/input/human-v-ai/5000human_5000machine.csv\n\n==================================================\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"csv_files = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if filename.endswith('.csv'):\n            csv_files.append(os.path.join(dirname, filename))\n\nif csv_files:\n    csv_path = csv_files[0]  # Use the first CSV found\n    print(f\"Loading: {csv_path}\")\nelse:\n    print(\"No CSV file found! Please add the dataset to your notebook.\")\n    print(\"Click 'Add Data' button and search for 'human-v-ai' or upload the dataset\")\n    csv_path = None\n\n# Load the data\nif csv_path:\n    df = pd.read_csv(csv_path)\nelse:\n    print(\"\\nPlease add the dataset first!\")\n    df = None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T20:19:42.652826Z","iopub.execute_input":"2025-10-31T20:19:42.653106Z","iopub.status.idle":"2025-10-31T20:19:43.602261Z","shell.execute_reply.started":"2025-10-31T20:19:42.653086Z","shell.execute_reply":"2025-10-31T20:19:43.600989Z"}},"outputs":[{"name":"stdout","text":"Loading: /kaggle/input/human-v-ai/5000human_5000machine.csv\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Step 2: Basic Information\nprint(\"=\" * 50)\nprint(\"DATASET OVERVIEW\")\nprint(\"=\" * 50)\nprint(f\"Total samples: {len(df)}\")\nprint(f\"Columns: {list(df.columns)}\")\nprint(f\"\\nData types:\")\nprint(df.dtypes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T20:19:43.603630Z","iopub.execute_input":"2025-10-31T20:19:43.603926Z","iopub.status.idle":"2025-10-31T20:19:43.611101Z","shell.execute_reply.started":"2025-10-31T20:19:43.603903Z","shell.execute_reply":"2025-10-31T20:19:43.610067Z"}},"outputs":[{"name":"stdout","text":"==================================================\nDATASET OVERVIEW\n==================================================\nTotal samples: 10000\nColumns: ['index', 'text', 'label', 'source', 'text_word_count', 'label_cat', 'source_int', 'processed_text']\n\nData types:\nindex               int64\ntext               object\nlabel               int64\nsource             object\ntext_word_count     int64\nlabel_cat          object\nsource_int          int64\nprocessed_text     object\ndtype: object\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Step 3: Check for missing values\nprint(\"\\n\" + \"=\" * 50)\nprint(\"MISSING VALUES\")\nprint(\"=\" * 50)\nprint(df.isnull().sum())\n\n# Step 4: View first few rows\nprint(\"\\n\" + \"=\" * 50)\nprint(\"FIRST 5 ROWS\")\nprint(\"=\" * 50)\nprint(df.head())\n\n# Step 5: Check the distribution of labels (Human vs AI)\nprint(\"\\n\" + \"=\" * 50)\nprint(\"LABEL DISTRIBUTION\")\nprint(\"=\" * 50)\n# This will depend on how the labels are stored in the CSV\n# Common column names: 'label', 'class', 'type', 'generated', 'source'\nfor col in df.columns:\n    if col.lower() in ['label', 'class', 'type', 'generated', 'source', 'is_ai', 'human']:\n        print(f\"\\n{col} distribution:\")\n        print(df[col].value_counts())\n\n# Step 6: Text statistics\nprint(\"\\n\" + \"=\" * 50)\nprint(\"TEXT STATISTICS\")\nprint(\"=\" * 50)\n# Find the text column\ntext_col = None\nfor col in df.columns:\n    if col.lower() in ['text', 'content', 'passage', 'essay', 'article']:\n        text_col = col\n        break\n\nif text_col:\n    df['text_length'] = df[text_col].astype(str).apply(len)\n    df['word_count'] = df[text_col].astype(str).apply(lambda x: len(x.split()))\n    \n    print(f\"Text length statistics:\")\n    print(df['text_length'].describe())\n    print(f\"\\nWord count statistics:\")\n    print(df['word_count'].describe())\n    \n    print(f\"\\nSample text (first 200 characters):\")\n    print(df[text_col].iloc[0][:200] + \"...\")\nelse:\n    print(\"Could not automatically identify text column\")\n    print(\"Available columns:\", list(df.columns))\n\n# Step 7: Export summary\nprint(\"\\n\" + \"=\" * 50)\nprint(\"DATASET READY FOR TESTING\")\nprint(\"=\" * 50)\nprint(f\"✓ Dataset loaded successfully\")\nprint(f\"✓ {len(df)} samples available for Winston AI testing\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T20:19:43.612136Z","iopub.execute_input":"2025-10-31T20:19:43.612455Z","iopub.status.idle":"2025-10-31T20:19:43.872553Z","shell.execute_reply.started":"2025-10-31T20:19:43.612425Z","shell.execute_reply":"2025-10-31T20:19:43.871661Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nMISSING VALUES\n==================================================\nindex              0\ntext               0\nlabel              0\nsource             0\ntext_word_count    0\nlabel_cat          0\nsource_int         0\nprocessed_text     0\ndtype: int64\n\n==================================================\nFIRST 5 ROWS\n==================================================\n   index                                               text  label  source  \\\n0     47  All the Love you do not see\\r\\n\\r\\nOn writing ...      0  medium   \n1     85  Reading, writing and displaying images\\r\\n\\r\\n...      0  medium   \n2    255  Why I Think “Target Audience” Doesn’t Always M...      0  medium   \n3    268  All sorts of books are published every year, a...      0  medium   \n4    315  Finding The Right Question Not Answer\\r\\n\\r\\nW...      0  medium   \n\n   text_word_count label_cat  source_int  \\\n0              236     human           0   \n1              386     human           0   \n2              323     human           0   \n3              176     human           0   \n4              264     human           0   \n\n                                      processed_text  \n0  love see write heart hope photo joshua coleman...  \n1  read write display image anything computer vis...  \n2  think target audience always matter much first...  \n3  sort book publish every year sort bookbuyers l...  \n4  find right question answer face problem one fi...  \n\n==================================================\nLABEL DISTRIBUTION\n==================================================\n\nlabel distribution:\nlabel\n0    5000\n1    5000\nName: count, dtype: int64\n\nsource distribution:\nsource\nmedium           3025\nRotten_Tomato    2635\nWMT-16           2365\nwikipedia        1975\nName: count, dtype: int64\n\n==================================================\nTEXT STATISTICS\n==================================================\nText length statistics:\ncount    10000.000000\nmean      1622.892600\nstd        773.029222\nmin        238.000000\n25%        966.000000\n50%       1716.500000\n75%       2223.000000\nmax      15171.000000\nName: text_length, dtype: float64\n\nWord count statistics:\ncount    10000.000000\nmean       274.620300\nstd        126.197044\nmin         51.000000\n25%        162.000000\n50%        286.000000\n75%        395.000000\nmax        639.000000\nName: word_count, dtype: float64\n\nSample text (first 200 characters):\nAll the Love you do not see\n\nOn writing with heart and hope\n\nPhoto by JOSHUA COLEMAN on Unsplash\n\nThe words I do not recognize are the ones my heart wrote, so feverish with hope every line feels...\n\n==================================================\nDATASET READY FOR TESTING\n==================================================\n✓ Dataset loaded successfully\n✓ 10000 samples available for Winston AI testing\n","output_type":"stream"}],"execution_count":7}]}